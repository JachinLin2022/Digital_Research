{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Optional, Union\n",
    "from peft import LoraConfig\n",
    "from dataclasses import asdict, dataclass, field\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import warnings\n",
    "import math\n",
    "import copy\n",
    "from enum import Enum\n",
    "from datasets import load_dataset\n",
    "from utils.prompter import Prompter\n",
    "import transformers\n",
    "import os\n",
    "import sys\n",
    "\n",
    "base_model = \"/root/llama-7b-hf\"  # the only required argument\n",
    "data_path = \"train_data_3_class_clean.jsonl\"\n",
    "output_dir = \"/root/autodl-tmp/output2s\"\n",
    "# training hyperparams\n",
    "batch_size = 128\n",
    "micro_batch_size = 4\n",
    "num_epochs = 1000\n",
    "learning_rate = 3e-4\n",
    "cutoff_len = 256\n",
    "val_set_size = 0\n",
    "# lora hyperparams\n",
    "lora_r = 8\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "lora_target_modules = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    "# llm hyperparams\n",
    "train_on_inputs = True  # if False, masks out inputs in loss\n",
    "add_eos_token = False\n",
    "group_by_length = False  # faster, but produces an odd training loss curve\n",
    "# wandb params\n",
    "wandb_project = \"\"\n",
    "wandb_run_name = \"\"\n",
    "wandb_watch = \"\"  # options: false | gradients | all\n",
    "wandb_log_model = \"\"  # options: false | true\n",
    "# resume_from_checkpoint = '/root/autodl-tmp/output/checkpoint-100'  # either training checkpoint or final adapter\n",
    "resume_from_checkpoint=None\n",
    "prompt_template_name = \"alpaca\"  # The prompt template to use, will default to alpaca.\n",
    "device_map = \"auto\"\n",
    "gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "ddp = world_size != 1\n",
    "use_wandb = len(wandb_project) > 0 or (\n",
    "    \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompter = Prompter(prompt_template_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=False,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    " )\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import random\n",
    "def is_bnb_available():\n",
    "    return importlib.util.find_spec(\"bitsandbytes\") is not None\n",
    "if is_bnb_available():\n",
    "    import bitsandbytes as bnb\n",
    "class LoraLayer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "    ):\n",
    "        self.r = {}\n",
    "        self.lora_alpha = {}\n",
    "        self.scaling = {}\n",
    "        \n",
    "        self.lora_dropout = nn.ModuleDict({})\n",
    "        # self.lora_A = nn.ModuleDict({})\n",
    "        # self.lora_B = nn.ModuleDict({})\n",
    "        # For Embedding layer\n",
    "        self.lora_embedding_A = nn.ParameterDict({})\n",
    "        self.lora_embedding_B = nn.ParameterDict({})\n",
    "        # Mark the weight as unmerged\n",
    "        self.merged = False\n",
    "        self.disable_adapters = False\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # self.gate = nn.Linear(in_features, 4)\n",
    "\n",
    "    def update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights):\n",
    "        self.r[adapter_name] = r\n",
    "        self.lora_alpha[adapter_name] = lora_alpha\n",
    "        if lora_dropout > 0.0:\n",
    "            lora_dropout_layer = nn.Dropout(p=lora_dropout)\n",
    "        else:\n",
    "            lora_dropout_layer = nn.Identity()\n",
    "\n",
    "        self.lora_dropout.update(nn.ModuleDict({adapter_name: lora_dropout_layer}))\n",
    "        # Actual trainable parameters\n",
    "        self.lora_num = 8\n",
    "        if r > 0:\n",
    "            # self.lora_A.update(nn.ModuleDict({adapter_name: nn.Linear(self.in_features, r, bias=False)}))\n",
    "            # self.lora_B.update(nn.ModuleDict({adapter_name: nn.Linear(r, self.out_features, bias=False)}))\n",
    "            self.lora_A = nn.ParameterDict()\n",
    "            self.lora_B = nn.ParameterDict()\n",
    "            for i in range(self.lora_num):\n",
    "                self.lora_A[str(i)] = nn.Parameter(self.weight.new_zeros((r, self.in_features)))\n",
    "                self.lora_B[str(i)] = nn.Parameter(self.weight.new_zeros((self.out_features, r)))\n",
    "            # add gate\n",
    "            self.lora_gate = nn.Parameter(self.weight.new_zeros((self.lora_num, self.in_features)))\n",
    "            self.scaling[adapter_name] = lora_alpha / r\n",
    "        if init_lora_weights:\n",
    "            self.reset_lora_parameters(adapter_name)\n",
    "        self.to(self.weight.device)\n",
    "\n",
    "    def update_layer_embedding(self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights):\n",
    "        self.r[adapter_name] = r\n",
    "        self.lora_alpha[adapter_name] = lora_alpha\n",
    "        if lora_dropout > 0.0:\n",
    "            lora_dropout_layer = nn.Dropout(p=lora_dropout)\n",
    "        else:\n",
    "            lora_dropout_layer = nn.Identity()\n",
    "\n",
    "        self.lora_dropout.update(nn.ModuleDict({adapter_name: lora_dropout_layer}))\n",
    "        # Actual trainable parameters\n",
    "        if r > 0:\n",
    "            self.lora_embedding_A.update(\n",
    "                nn.ParameterDict({adapter_name: nn.Parameter(self.weight.new_zeros((r, self.in_features)))})\n",
    "            )\n",
    "            self.lora_embedding_B.update(\n",
    "                nn.ParameterDict({adapter_name: nn.Parameter(self.weight.new_zeros((self.out_features, r)))})\n",
    "            )\n",
    "            self.scaling[adapter_name] = lora_alpha / r\n",
    "        if init_lora_weights:\n",
    "            self.reset_lora_parameters(adapter_name)\n",
    "        self.to(self.weight.device)\n",
    "\n",
    "    def reset_lora_parameters(self, adapter_name):\n",
    "        if adapter_name in self.lora_A.keys():\n",
    "            # initialize A the same way as the default for nn.Linear and B to zero\n",
    "            # nn.init.kaiming_uniform_(self.lora_A[adapter_name].weight, a=math.sqrt(5))\n",
    "            # nn.init.zeros_(self.lora_B[adapter_name].weight)\n",
    "            nn.init.kaiming_uniform_(self.lora_A[adapter_name], a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_B[adapter_name])\n",
    "        if adapter_name in self.lora_embedding_A.keys():\n",
    "            # initialize a the same way as the default for nn.linear and b to zero\n",
    "            nn.init.zeros_(self.lora_embedding_A[adapter_name])\n",
    "            nn.init.normal_(self.lora_embedding_B[adapter_name])\n",
    "\n",
    "def transpose(weight, fan_in_fan_out):\n",
    "    return weight.T if fan_in_fan_out else weight\n",
    "\n",
    "class SuperScalableLinear(torch.nn.Linear):\n",
    "    def __init__(self, in_features, out_features, rank):\n",
    "        super(SuperScalableLinear, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        config_A_B = [f'LoRA_{rank}', 'vector', 'constant', 'none']\n",
    "        config_C = [f'LoRA_{rank}', 'vector', 'none']\n",
    "        config_D_E = ['constant', 'none', 'vector']\n",
    "        self.configs = []\n",
    "        for A in config_A_B:\n",
    "            for B in config_A_B:\n",
    "                for C in config_C:\n",
    "                    for D in config_D_E:\n",
    "                        for E in config_D_E:\n",
    "                            config = {'A':A,'B':B,'C':C,'D':D,'E':E}\n",
    "                            self.configs.append(config)\n",
    "\n",
    "        self.Ad, self.Au = self.make_param((out_features, in_features), f'LoRA_{rank}')\n",
    "        self.Bd, self.Bu = self.make_param((out_features, in_features), f'LoRA_{rank}')\n",
    "        self.Cd, self.Cu = self.make_param((in_features, 1), f'LoRA_{rank}')\n",
    "        self.D = nn.Parameter(torch.zeros(out_features))\n",
    "        self.E = nn.Parameter(torch.zeros(out_features))\n",
    "        self.eval_config = None\n",
    "        nn.init.xavier_uniform_(self.Au)\n",
    "        nn.init.xavier_uniform_(self.Bu)\n",
    "        nn.init.xavier_uniform_(self.Cu)\n",
    "        self.to(self.weight.device)\n",
    "    \n",
    "    def prepare_path(self, config, Xd, Xu=None):\n",
    "        if Xu is not None:\n",
    "            if 'LoRA' in config:\n",
    "                rank = int(config.split('_')[1])\n",
    "                X = torch.matmul(Xd[:,:rank], Xu[:rank, :])\n",
    "            elif 'vector' in config:\n",
    "                X = Xd[:,0].unsqueeze(1)\n",
    "            elif 'constant' in config:\n",
    "                X = Xd[0,0]\n",
    "            elif 'none' in config:\n",
    "                X = torch.zeros(Xd.shape[0], Xu.shape[1]).cuda()\n",
    "            else:\n",
    "                raise ValueError\n",
    "        else:\n",
    "            if 'vector' in config:\n",
    "                X = Xd\n",
    "            elif 'constant' in config:\n",
    "                X = Xd[0]\n",
    "            elif 'none' in config:\n",
    "                X = torch.zeros(1).cuda()\n",
    "            else:\n",
    "                raise ValueError\n",
    "        return X\n",
    "    \n",
    "    def make_param(self, shape, config=None):\n",
    "        if 'LoRA' in config:\n",
    "            out_feature = shape[0]\n",
    "            in_feature = shape[1]\n",
    "            try:\n",
    "                rank = int(config.split('_')[1])\n",
    "            except:\n",
    "                rank = 4\n",
    "            return nn.Parameter(torch.zeros(out_feature, rank)), nn.Parameter(torch.zeros(rank, in_feature))\n",
    "        return nn.Parameter(torch.zeros(*shape))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        if self.eval_config is not None:\n",
    "            path_config = self.eval_config\n",
    "        else:\n",
    "            path_config = random.choice(self.configs)\n",
    "\n",
    "        previous_dtype = input.dtype\n",
    "        \n",
    "        A = self.prepare_path(path_config['A'], self.Ad, self.Au)\n",
    "        B = self.prepare_path(path_config['B'], self.Bd, self.Bu)\n",
    "        C = self.prepare_path(path_config['C'], self.Cd, self.Cu)\n",
    "        D = self.prepare_path(path_config['D'], self.D)\n",
    "        E = self.prepare_path(path_config['E'], self.E)\n",
    "        optimal_weight = self.weight + self.weight*A + B\n",
    "        if torch.is_tensor(self.bias):\n",
    "            optimal_bias = self.bias + self.bias*D + E\n",
    "        else:\n",
    "            optimal_bias = E\n",
    "        optimal_prompt = torch.matmul(self.weight, C).squeeze()\n",
    "        input = input.to(optimal_weight.dtype)\n",
    "        result = F.linear(input, optimal_weight, optimal_bias+optimal_prompt)\n",
    "        result = result.to(previous_dtype)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def from_linear(linear_module, rank):\n",
    "        new_linear = SuperScalableLinear(linear_module.in_features, linear_module.out_features, rank)\n",
    "        new_linear.weight = linear_module.weight\n",
    "        new_linear.bias = linear_module.bias\n",
    "        return new_linear\n",
    "    \n",
    "class Linear(nn.Linear, LoraLayer):\n",
    "    # Lora implemented in a dense layer\n",
    "    def __init__(\n",
    "        self,\n",
    "        adapter_name: str,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        r: int = 0,\n",
    "        lora_alpha: int = 1,\n",
    "        lora_dropout: float = 0.0,\n",
    "        fan_in_fan_out: bool = False,  # Set this to True if the layer to replace stores weight like (fan_in, fan_out)\n",
    "        **kwargs,\n",
    "    ):\n",
    "        init_lora_weights = kwargs.pop(\"init_lora_weights\", True)\n",
    "\n",
    "        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n",
    "        LoraLayer.__init__(self, in_features=in_features, out_features=out_features)\n",
    "        # Freezing the pre-trained weight matrix\n",
    "        self.weight.requires_grad = False\n",
    "\n",
    "        self.fan_in_fan_out = fan_in_fan_out\n",
    "        if fan_in_fan_out:\n",
    "            self.weight.data = self.weight.data.T\n",
    "\n",
    "        nn.Linear.reset_parameters(self)\n",
    "        self.update_layer(adapter_name, r, lora_alpha, lora_dropout, init_lora_weights)\n",
    "        self.active_adapter = adapter_name\n",
    "\n",
    "    def merge(self):\n",
    "        if self.active_adapter not in self.lora_A.keys():\n",
    "            return\n",
    "        if self.merged:\n",
    "            warnings.warn(\"Already merged. Nothing to do.\")\n",
    "            return\n",
    "        if self.r[self.active_adapter] > 0:\n",
    "            self.weight.data += (\n",
    "                transpose(\n",
    "                    self.lora_B[self.active_adapter].weight @ self.lora_A[self.active_adapter].weight,\n",
    "                    self.fan_in_fan_out,\n",
    "                )\n",
    "                * self.scaling[self.active_adapter]\n",
    "            )\n",
    "            self.merged = True\n",
    "\n",
    "    def unmerge(self):\n",
    "        if self.active_adapter not in self.lora_A.keys():\n",
    "            return\n",
    "        if not self.merged:\n",
    "            warnings.warn(\"Already unmerged. Nothing to do.\")\n",
    "            return\n",
    "        if self.r[self.active_adapter] > 0:\n",
    "            self.weight.data -= (\n",
    "                transpose(\n",
    "                    self.lora_B[self.active_adapter].weight @ self.lora_A[self.active_adapter].weight,\n",
    "                    self.fan_in_fan_out,\n",
    "                )\n",
    "                * self.scaling[self.active_adapter]\n",
    "            )\n",
    "            self.merged = False\n",
    "\n",
    "    def compute_average_weights(self, module_dict):\n",
    "        weights = []\n",
    "        for module in module_dict.values():\n",
    "            for param in module.parameters():\n",
    "                weights.append(param)\n",
    "\n",
    "        num_parameters = len(weights)\n",
    "        if num_parameters > 0:\n",
    "            average_weight = torch.stack(weights).mean(dim=0)\n",
    "        else:\n",
    "            average_weight = None\n",
    "\n",
    "        return average_weight\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        previous_dtype = x.dtype\n",
    "\n",
    "        if self.active_adapter not in self.lora_A.keys():\n",
    "            return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "        if self.disable_adapters:\n",
    "            if self.r[self.active_adapter] > 0 and self.merged:\n",
    "                self.unmerge()\n",
    "            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "        elif self.r[self.active_adapter] > 0 and not self.merged:\n",
    "            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "\n",
    "            # x = x.to(self.lora_A[self.active_adapter].weight.dtype)\n",
    "            x = x.to(self.lora_A[self.active_adapter].dtype)\n",
    "\n",
    "            x_s = x[:,0,:]\n",
    "            t = x_s @ self.lora_gate.T\n",
    "            gate = torch.argmax(t,dim=1)\n",
    "            x_gate_dict = {}\n",
    "            x_gate_output_dict = {}\n",
    "            # init\n",
    "            for i in range(self.lora_num):\n",
    "                x_gate_dict[str(i)] = torch.empty((0, x.shape[1], x.shape[2])).to(x.device)\n",
    "            # dispatch\n",
    "            for i in range(x.shape[0]):\n",
    "                x_gate_dict[str(gate[i].item())] = torch.cat((x_gate_dict[str(gate[i].item())], x[i].unsqueeze(0)), dim=0)\n",
    "            # forward\n",
    "            for i in range(self.lora_num):\n",
    "                x_gate_output_dict[str(i)] = (self.lora_dropout[self.active_adapter](x_gate_dict[str(i)]) @ self.lora_A[str(i)].T @ self.lora_B[str(i)].T) * self.scaling[self.active_adapter]\n",
    "            \n",
    "            result_restored = torch.zeros_like(result).to(result.device)\n",
    "            # init index                \n",
    "            idx_list = {}\n",
    "            for i in range(self.lora_num):\n",
    "                idx_list[str(i)] = 0\n",
    "            # merge    \n",
    "            for i in range(x.shape[0]):\n",
    "                result_restored[i] = x_gate_output_dict[str(gate[i].item())][idx_list[str(gate[i].item())]]\n",
    "                idx_list[str(gate[i].item())] += 1\n",
    "            result += result_restored\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # gate_output = self.gate(x)\n",
    "\n",
    "            # probabilities = torch.softmax(gate_output, dim=1)\n",
    "            # # 计算每个样本概率总和并获取最大概率的位置索引\n",
    "            # sum_probs = torch.sum(probabilities, dim=1)\n",
    "            # max_prob_idx = torch.argmax(sum_probs, dim=0)\n",
    "\n",
    "            # result += (\n",
    "            #         self.lora_B[str(max_prob_idx.item())](\n",
    "            #             self.lora_A[str(max_prob_idx.item())](self.lora_dropout[str(max_prob_idx.item())](x))\n",
    "            #         )\n",
    "            #         * self.scaling[str(max_prob_idx.item())]\n",
    "            #     )\n",
    "            \n",
    "            # if self.training:\n",
    "            #     module_keys = list(self.lora_A.keys())\n",
    "            #     random_key = random.choice(module_keys)\n",
    "            #     result += (\n",
    "            #         self.lora_B[random_key](\n",
    "            #             self.lora_A[random_key](self.lora_dropout[random_key](x))\n",
    "            #         )\n",
    "            #         * self.scaling[random_key]\n",
    "            #     )\n",
    "            # else:\n",
    "            #     # average_dropout_weights = self.compute_average_weights(self.lora_dropout)\n",
    "            #     average_A_weights = self.compute_average_weights(self.lora_A)\n",
    "            #     average_B_weights = self.compute_average_weights(self.lora_B)\n",
    "            #     module_keys = list(self.lora_A.keys())\n",
    "            #     random_key = random.choice(module_keys)\n",
    "            #     average_scaling = self.scaling[random_key]\n",
    "                    \n",
    "            #     result += F.linear(F.linear(self.lora_dropout[random_key](x),average_A_weights),average_B_weights)*average_scaling\n",
    "\n",
    "            # for key in self.lora_A.keys(): \n",
    "            #     result += (\n",
    "            #         self.lora_B[key](\n",
    "            #             self.lora_A[key](self.lora_dropout[key](x))\n",
    "            #         )\n",
    "            #         * self.scaling[key]\n",
    "            #     )\n",
    "            # result += (\n",
    "            #     self.lora_B[self.active_adapter](\n",
    "            #         self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))\n",
    "            #     )\n",
    "            #     * self.scaling[self.active_adapter]\n",
    "            # )\n",
    "        else:\n",
    "            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "\n",
    "        result = result.to(previous_dtype)\n",
    "\n",
    "        return result\n",
    "    \n",
    "def _freeze_adapter(model, adapter_name):\n",
    "    for n, p in model.named_parameters():\n",
    "        if adapter_name in n:\n",
    "            p.requires_grad = False\n",
    "\n",
    "def mark_only_lora_as_trainable(model: nn.Module, bias: str = \"none\") -> None:\n",
    "    for n, p in model.named_parameters():\n",
    "        if \"lora_\" not in n:\n",
    "            p.requires_grad = False\n",
    "        # if any([x in n for x in ['A', 'B', 'C', 'D', 'E']]):\n",
    "        #     p.requires_grad = True\n",
    "        # else:\n",
    "        #     p.requires_grad = False\n",
    "    if bias == \"none\":\n",
    "        return\n",
    "    elif bias == \"all\":\n",
    "        for n, p in model.named_parameters():\n",
    "            if \"bias\" in n:\n",
    "                p.requires_grad = True\n",
    "    elif bias == \"lora_only\":\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, LoraLayer) and hasattr(m, \"bias\") and m.bias is not None:\n",
    "                m.bias.requires_grad = True\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "def _get_submodules(model, key):\n",
    "    parent = model.get_submodule(\".\".join(key.split(\".\")[:-1]))\n",
    "    target_name = key.split(\".\")[-1]\n",
    "    target = model.get_submodule(key)\n",
    "    return parent, target, target_name\n",
    "\n",
    "\n",
    "class ModulesToSaveWrapper(torch.nn.Module):\n",
    "    def __init__(self, module_to_save, adapter_name):\n",
    "        super().__init__()\n",
    "        self.original_module = module_to_save\n",
    "        self.modules_to_save = torch.nn.ModuleDict({})\n",
    "        self.update(adapter_name)\n",
    "        self.active_adapter = adapter_name\n",
    "\n",
    "    def update(self, adapter_name):\n",
    "        self.modules_to_save.update(torch.nn.ModuleDict({adapter_name: copy.deepcopy(self.original_module)}))\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        if self.active_adapter not in self.modules_to_save:\n",
    "            return self.original_module(*args, **kwargs)\n",
    "        return self.modules_to_save[self.active_adapter](*args, **kwargs)\n",
    "    \n",
    "class LoraModel(torch.nn.Module):\n",
    "    def __init__(self, model, config, adapter_name):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.forward = self.model.forward\n",
    "        self.peft_config = config\n",
    "        self.add_adapter(adapter_name, self.peft_config[adapter_name])\n",
    "                \n",
    "    def add_adapter(self, adapter_name, config=None):\n",
    "        if config is not None:\n",
    "            model_config = self.model.config.to_dict() if hasattr(self.model.config, \"to_dict\") else self.model.config\n",
    "            config = self._prepare_lora_config(config, model_config)\n",
    "            self.peft_config[adapter_name] = config\n",
    "        self._find_and_replace(adapter_name)\n",
    "        if len(self.peft_config) > 1 and self.peft_config[adapter_name].bias != \"none\":\n",
    "            raise ValueError(\n",
    "                \"LoraModel supports only 1 adapter with bias. When using multiple adapters, set bias to 'none' for all adapters.\"\n",
    "            )\n",
    "        mark_only_lora_as_trainable(self.model, self.peft_config[adapter_name].bias)\n",
    "        if self.peft_config[adapter_name].inference_mode:\n",
    "            _freeze_adapter(self.model, adapter_name)\n",
    "\n",
    "    def _find_and_replace(self, adapter_name):\n",
    "        lora_config = self.peft_config[adapter_name]\n",
    "        loaded_in_8bit = getattr(self.model, \"is_loaded_in_8bit\", False)\n",
    "        if loaded_in_8bit and not is_bnb_available():\n",
    "            raise ImportError(\n",
    "                \"To use Lora with 8-bit quantization, please install the `bitsandbytes` package. \"\n",
    "                \"You can install it with `pip install bitsandbytes`.\"\n",
    "            )\n",
    "        is_target_modules_in_base_model = False\n",
    "        kwargs = {\n",
    "            \"r\": lora_config.r,\n",
    "            \"lora_alpha\": lora_config.lora_alpha,\n",
    "            \"lora_dropout\": lora_config.lora_dropout,\n",
    "            \"fan_in_fan_out\": lora_config.fan_in_fan_out,\n",
    "            \"init_lora_weights\": lora_config.init_lora_weights,\n",
    "        }\n",
    "        key_list = [key for key, _ in self.model.named_modules()]\n",
    "        for key in key_list:\n",
    "            # if 'head' in key:\n",
    "            #     print(key)\n",
    "            #     continue\n",
    "            # parent, target, target_name = _get_submodules(self.model, key)\n",
    "            # if isinstance(target, torch.nn.Linear):\n",
    "            #     is_target_modules_in_base_model = True\n",
    "            #     in_features, out_features = target.in_features, target.out_features\n",
    "            #     new_module = SuperScalableLinear(in_features, out_features, lora_config.r)\n",
    "            #     self._replace_module(parent, target_name, new_module, target)\n",
    "            # continue\n",
    "\n",
    "            if isinstance(lora_config.target_modules, str):\n",
    "                target_module_found = re.fullmatch(lora_config.target_modules, key)\n",
    "            else:\n",
    "                target_module_found = any(key.endswith(target_key) for target_key in lora_config.target_modules)\n",
    "            if target_module_found:\n",
    "                if not is_target_modules_in_base_model:\n",
    "                    is_target_modules_in_base_model = True\n",
    "                parent, target, target_name = _get_submodules(self.model, key)\n",
    "                if hasattr(target, \"bias\"):\n",
    "                    bias = target.bias is not None\n",
    "\n",
    "                if isinstance(target, LoraLayer):\n",
    "                    target.update_layer(\n",
    "                        adapter_name,\n",
    "                        lora_config.r,\n",
    "                        lora_config.lora_alpha,\n",
    "                        lora_config.lora_dropout,\n",
    "                        lora_config.init_lora_weights,\n",
    "                    )\n",
    "                else:\n",
    "                    if isinstance(target, torch.nn.Linear):\n",
    "                        in_features, out_features = target.in_features, target.out_features\n",
    "                        if kwargs[\"fan_in_fan_out\"]:\n",
    "                            warnings.warn(\n",
    "                                \"fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. \"\n",
    "                                \"Setting fan_in_fan_out to False.\"\n",
    "                            )\n",
    "                            kwargs[\"fan_in_fan_out\"] = lora_config.fan_in_fan_out = False\n",
    "                    new_module = Linear(adapter_name, in_features, out_features, bias=bias, **kwargs)\n",
    "                    # new_module = SuperScalableLinear(in_features, out_features, lora_config.r)\n",
    "                    # if loaded_in_8bit and isinstance(target, bnb.nn.Linear8bitLt):\n",
    "                    #     eightbit_kwargs = kwargs.copy()\n",
    "                    #     eightbit_kwargs.update(\n",
    "                    #         {\n",
    "                    #             \"has_fp16_weights\": target.state.has_fp16_weights,\n",
    "                    #             \"memory_efficient_backward\": target.state.memory_efficient_backward,\n",
    "                    #             \"threshold\": target.state.threshold,\n",
    "                    #             \"index\": target.index,\n",
    "                    #         }\n",
    "                    #     )\n",
    "                    #     new_module = Linear8bitLt(\n",
    "                    #         adapter_name, target.in_features, target.out_features, bias=bias, **eightbit_kwargs\n",
    "                    #     )\n",
    "                    # elif isinstance(target, torch.nn.Embedding):\n",
    "                    #     embedding_kwargs = kwargs.copy()\n",
    "                    #     embedding_kwargs.pop(\"fan_in_fan_out\", None)\n",
    "                    #     in_features, out_features = target.num_embeddings, target.embedding_dim\n",
    "                    #     new_module = Embedding(adapter_name, in_features, out_features, **embedding_kwargs)\n",
    "                    # else:\n",
    "                    #     if isinstance(target, torch.nn.Linear):\n",
    "                    #         in_features, out_features = target.in_features, target.out_features\n",
    "                    #         if kwargs[\"fan_in_fan_out\"]:\n",
    "                    #             warnings.warn(\n",
    "                    #                 \"fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. \"\n",
    "                    #                 \"Setting fan_in_fan_out to False.\"\n",
    "                    #             )\n",
    "                    #             kwargs[\"fan_in_fan_out\"] = lora_config.fan_in_fan_out = False\n",
    "                    #     elif isinstance(target, Conv1D):\n",
    "                    #         in_features, out_features = (\n",
    "                    #             target.weight.ds_shape if hasattr(target.weight, \"ds_shape\") else target.weight.shape\n",
    "                    #         )\n",
    "                    #         if not kwargs[\"fan_in_fan_out\"]:\n",
    "                    #             warnings.warn(\n",
    "                    #                 \"fan_in_fan_out is set to False but the target module is `Conv1D`. \"\n",
    "                    #                 \"Setting fan_in_fan_out to True.\"\n",
    "                    #             )\n",
    "                    #             kwargs[\"fan_in_fan_out\"] = lora_config.fan_in_fan_out = True\n",
    "                    #     else:\n",
    "                    #         raise ValueError(\n",
    "                    #             f\"Target module {target} is not supported. \"\n",
    "                    #             f\"Currently, only `torch.nn.Linear` and `Conv1D` are supported.\"\n",
    "                    #         )\n",
    "                    #     new_module = Linear(adapter_name, in_features, out_features, bias=bias, **kwargs)\n",
    "\n",
    "                    self._replace_module(parent, target_name, new_module, target)\n",
    "        if not is_target_modules_in_base_model:\n",
    "            raise ValueError(\n",
    "                f\"Target modules {lora_config.target_modules} not found in the base model. \"\n",
    "                f\"Please check the target modules and try again.\"\n",
    "            )\n",
    "\n",
    "    def _replace_module(self, parent_module, child_name, new_module, old_module):\n",
    "        setattr(parent_module, child_name, new_module)\n",
    "        new_module.weight = old_module.weight\n",
    "        if hasattr(old_module, \"bias\"):\n",
    "            if old_module.bias is not None:\n",
    "                new_module.bias = old_module.bias\n",
    "\n",
    "        if getattr(old_module, \"state\", None) is not None:\n",
    "            new_module.state = old_module.state\n",
    "            new_module.to(old_module.weight.device)\n",
    "\n",
    "        # dispatch to correct device\n",
    "        for name, module in new_module.named_modules():\n",
    "            if \"lora_\" in name:\n",
    "                module.to(old_module.weight.device)\n",
    "            if any([x in name for x in ['A', 'B', 'C', 'D', 'E', 'head']]):\n",
    "                module.to(old_module.weight.device)\n",
    "\n",
    "    def __getattr__(self, name: str):\n",
    "        \"\"\"Forward missing attributes to the wrapped module.\"\"\"\n",
    "        try:\n",
    "            return super().__getattr__(name)  # defer to nn.Module's logic\n",
    "        except AttributeError:\n",
    "            return getattr(self.model, name)\n",
    "\n",
    "    def get_peft_config_as_dict(self, inference: bool = False):\n",
    "        config_dict = {}\n",
    "        for key, value in self.peft_config.items():\n",
    "            config = {k: v.value if isinstance(v, Enum) else v for k, v in asdict(value).items()}\n",
    "            if inference:\n",
    "                config[\"inference_mode\"] = True\n",
    "        config_dict[key] = config\n",
    "        return config\n",
    "\n",
    "    def _set_adapter_layers(self, enabled=True):\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, LoraLayer):\n",
    "                module.disable_adapters = False if enabled else True\n",
    "\n",
    "    def enable_adapter_layers(self):\n",
    "        self._set_adapter_layers(enabled=True)\n",
    "\n",
    "    def disable_adapter_layers(self):\n",
    "        self._set_adapter_layers(enabled=False)\n",
    "\n",
    "    def set_adapter(self, adapter_name):\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, LoraLayer):\n",
    "                if module.merged:\n",
    "                    warnings.warn(\"Adapter cannot be set when the model is merged. Unmerging the model first.\")\n",
    "                    module.unmerge()\n",
    "                module.active_adapter = adapter_name\n",
    "\n",
    "    def merge_adapter(self):\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, LoraLayer):\n",
    "                module.merge()\n",
    "\n",
    "    def unmerge_adapter(self):\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, LoraLayer):\n",
    "                module.unmerge()\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_lora_config(peft_config, model_config):\n",
    "        # if peft_config.target_modules is None:\n",
    "        #     if model_config[\"model_type\"] not in TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING:\n",
    "        #         raise ValueError(\"Please specify `target_modules` in `peft_config`\")\n",
    "        #     peft_config.target_modules = TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING[model_config[\"model_type\"]]\n",
    "        if peft_config.inference_mode:\n",
    "            peft_config.merge_weights = True\n",
    "        return peft_config\n",
    "\n",
    "    def merge_and_unload(self):\n",
    "        r\"\"\"\n",
    "        This method merges the LoRa layers into the base model. This is needed if someone wants to use the base model\n",
    "        as a standalone model.\n",
    "        \"\"\"\n",
    "        if getattr(self.config, \"model_type\", None) == \"gpt2\":\n",
    "            raise ValueError(\"GPT2 models are not supported for merging LORA layers\")\n",
    "\n",
    "        if getattr(self.model, \"is_loaded_in_8bit\", False):\n",
    "            raise ValueError(\"Cannot merge LORA layers when the model is loaded in 8-bit mode\")\n",
    "\n",
    "        key_list = [key for key, _ in self.model.named_modules() if \"lora\" not in key]\n",
    "        for key in key_list:\n",
    "            try:\n",
    "                parent, target, target_name = _get_submodules(self.model, key)\n",
    "            except AttributeError:\n",
    "                continue\n",
    "            if isinstance(target, LoraLayer):\n",
    "                bias = target.bias is not None\n",
    "                new_module = torch.nn.Linear(target.in_features, target.out_features, bias=bias)\n",
    "                target.merge()\n",
    "                self._replace_module(parent, target_name, new_module, target)\n",
    "\n",
    "            # save any additional trainable modules part of `modules_to_save`\n",
    "            if isinstance(target, ModulesToSaveWrapper):\n",
    "                setattr(parent, target_name, target.modules_to_save[target.active_adapter])\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def add_weighted_adapter(self, adapters, weights, adapter_name):\n",
    "        if len({self.peft_config[adapter].r for adapter in adapters}) != 1:\n",
    "            raise ValueError(\"All adapters must have the same r value\")\n",
    "        self.peft_config[adapter_name] = self.peft_config[adapters[0]]\n",
    "        self.peft_config[adapter_name].lora_alpha = self.peft_config[adapters[0]].r\n",
    "        self._find_and_replace(adapter_name)\n",
    "        mark_only_lora_as_trainable(self.model, self.peft_config[adapter_name].bias)\n",
    "        _freeze_adapter(self.model, adapter_name)\n",
    "        key_list = [key for key, _ in self.model.named_modules() if \"lora\" not in key]\n",
    "        for key in key_list:\n",
    "            _, target, _ = _get_submodules(self.model, key)\n",
    "            if isinstance(target, LoraLayer):\n",
    "                if adapter_name in target.lora_A:\n",
    "                    target.lora_A[adapter_name].weight.data = target.lora_A[adapter_name].weight.data * 0.0\n",
    "                    target.lora_B[adapter_name].weight.data = target.lora_B[adapter_name].weight.data * 0.0\n",
    "                    for adapter, weight in zip(adapters, weights):\n",
    "                        if adapter not in target.lora_A:\n",
    "                            continue\n",
    "                        target.lora_A[adapter_name].weight.data += (\n",
    "                            target.lora_A[adapter].weight.data * weight * target.scaling[adapter]\n",
    "                        )\n",
    "                        target.lora_B[adapter_name].weight.data += target.lora_B[adapter].weight.data * weight\n",
    "\n",
    "                elif adapter_name in target.lora_embedding_A:\n",
    "                    target.lora_embedding_A[adapter_name].data = target.lora_embedding_A[adapter_name].data * 0.0\n",
    "                    target.lora_embedding_B[adapter_name].data = target.lora_embedding_B[adapter_name].data * 0.0\n",
    "                    for adapter, weight in zip(adapters, weights):\n",
    "                        if adapter not in target.lora_embedding_A:\n",
    "                            continue\n",
    "                        target.lora_embedding_A[adapter_name].data += (\n",
    "                            target.lora_embedding_A[adapter].data * weight * target.scaling[adapter]\n",
    "                        )\n",
    "                        target.lora_embedding_B[adapter_name].data += target.lora_embedding_B[adapter].data * weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peft_config = {}\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    # r=4,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=lora_target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "# model = get_peft_model(model, config)\n",
    "\n",
    "peft_config['0'] = config\n",
    "\n",
    "model = LoraModel(model, peft_config, '0')\n",
    "\n",
    "\n",
    "# config2 = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32,\n",
    "#     target_modules=lora_target_modules,\n",
    "#     lora_dropout=lora_dropout,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     inference_mode = False\n",
    "# )\n",
    "# peft_config['default2'] = config\n",
    "# for i in range(3):\n",
    "#     model.add_adapter(f\"{i+1}\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key, _ in model.named_modules():\n",
    "    pass\n",
    "    # print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    # print(name, param.requires_grad,param.dtype)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(prompt, add_eos_token=True):\n",
    "        # there's probably a way to do this with the tokenizer settings\n",
    "        # but again, gotta move fast\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        if (\n",
    "            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < cutoff_len\n",
    "            and add_eos_token\n",
    "        ):\n",
    "            result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "            result[\"attention_mask\"].append(1)\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "        return result\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    data_point[\"instruction\"] = 'What is the sentiment toward Bitcoin in the input sentence? [positive, negative, neutral]'\n",
    "    data_point[\"input\"] = data_point['text']\n",
    "    data_point[\"output\"] = data_point['label']\n",
    "    del data_point['text']\n",
    "    del data_point['label']\n",
    "    full_prompt = prompter.generate_prompt(\n",
    "        data_point[\"instruction\"],\n",
    "        data_point[\"input\"],\n",
    "        data_point[\"output\"],\n",
    "    )\n",
    "    # print(full_prompt)\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    if not train_on_inputs:\n",
    "        user_prompt = prompter.generate_prompt(\n",
    "            data_point[\"instruction\"], data_point[\"input\"]\n",
    "        )\n",
    "        tokenized_user_prompt = tokenize(\n",
    "            user_prompt, add_eos_token=add_eos_token\n",
    "        )\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        if add_eos_token:\n",
    "            user_prompt_len -= 1\n",
    "\n",
    "        tokenized_full_prompt[\"labels\"] = [\n",
    "            -100\n",
    "        ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "            user_prompt_len:\n",
    "        ]  # could be sped up, probably\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = load_dataset(\"json\", data_files=data_path)\n",
    "\n",
    "data[\"test\"] = data[\"train\"].select(range(300,len(data[\"train\"])))\n",
    "data[\"train\"] = data[\"train\"].select(range(300))\n",
    "# print(data[\"train\"][0])\n",
    "train_data = data[\"train\"].map(generate_and_tokenize_prompt)\n",
    "val_data = data[\"test\"].map(generate_and_tokenize_prompt)\n",
    "val_set_size = len(val_data)\n",
    "# val_set_size = 0\n",
    "print(train_data)\n",
    "print(val_set_size, val_data)\n",
    "# train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "def my_evaluate(self, ignore_keys):\n",
    "    self.model.eval()    \n",
    "    instructions = []\n",
    "    with open(data_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            data_point = {}\n",
    "            data_point[\"instruction\"] = 'What is the sentiment toward Bitcoin in the input sentence? [positive, negative, neutral]'\n",
    "            data_point[\"input\"] = data['text']\n",
    "            data_point[\"output\"] = data['label']\n",
    "            full_prompt = prompter.generate_prompt(\n",
    "                data_point[\"instruction\"],\n",
    "                data_point[\"input\"]\n",
    "            )\n",
    "            instructions.append({'context':full_prompt, 'target':data['label']})\n",
    "\n",
    "        # print(instructions[0])\n",
    "        start_time = time.time()\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            with torch.no_grad():\n",
    "                right = 0\n",
    "                all = 0\n",
    "                batch_size = 32\n",
    "                input_texts = []\n",
    "                targets = []\n",
    "                for idx, item in enumerate(instructions[300:]):\n",
    "                    # feature = format_example(item)\n",
    "                    # input_text = feature[\"context\"]\n",
    "                    all = all + 1\n",
    "                    input_texts.append(item[\"context\"])\n",
    "                    targets.append(item[\"target\"])\n",
    "                test_loader = DataLoader(input_texts, batch_size=batch_size)\n",
    "                for batch_idx,batch in enumerate(test_loader):\n",
    "                    input_ids = tokenizer(batch, padding=True,return_tensors='pt').to('cuda')\n",
    "                    out = model.generate(\n",
    "                        **input_ids,\n",
    "                        temperature=0,\n",
    "                        return_dict_in_generate= True,\n",
    "                        output_scores=True,\n",
    "                        max_new_tokens = 1\n",
    "                    )\n",
    "                    seqs = out['sequences']\n",
    "                    scores = out['scores']\n",
    "                    # print(scores[0].shape)\n",
    "                    results = tokenizer.batch_decode(seqs)\n",
    "                    # print(results, '\\n\\n')\n",
    "                    # break\n",
    "                    for idx,res in enumerate(results):\n",
    "                        pred = res[res.find('Response') + 10:]\n",
    "                        target = targets[batch_idx*batch_size + idx]\n",
    "                        if target.find(pred) >= 0:\n",
    "                            right = right + 1\n",
    "                    # print(right,all,right/all)\n",
    "    metrics = {\"eval_acc\": right/all}\n",
    "    self.log(metrics)\n",
    "    # print(metrics)\n",
    "    self.model.train()\n",
    "    self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, metrics)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "transformers.Trainer.evaluate = my_evaluate\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=micro_batch_size,\n",
    "        gradient_accumulation_steps=1,\n",
    "        # warmup_steps=100,\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        fp16=True,\n",
    "        # logging_strategy  = \"steps\",\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_torch\",\n",
    "        metric_for_best_model = \"acc\",\n",
    "        evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "        save_strategy=\"no\",\n",
    "        eval_steps=100 if val_set_size > 0 else None,\n",
    "        save_steps=100,\n",
    "        output_dir=output_dir,\n",
    "        save_total_limit=50,\n",
    "        # load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "        ddp_find_unused_parameters=False if ddp else None,\n",
    "        group_by_length=group_by_length,\n",
    "        report_to=\"wandb\" if use_wandb else None,\n",
    "        run_name=wandb_run_name if use_wandb else None,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# old_state_dict = model.state_dict\n",
    "# model.state_dict = (\n",
    "#     lambda self, *_, **__: get_peft_model_state_dict(\n",
    "#         self, old_state_dict()\n",
    "#     )\n",
    "# ).__get__(model, type(model))\n",
    "# if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "#     model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=resume_from_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "# from avalanche.evaluation.metrics.accuracy import Accuracy\n",
    "\n",
    "class EvolutionSearcher(object):\n",
    "\n",
    "    def __init__(self, args, model, choices, output_dir):\n",
    "        # self.device = device\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "        self.max_epochs = args.max_epochs\n",
    "        self.select_num = args.select_num\n",
    "        self.population_num = args.population_num\n",
    "        self.m_prob = args.m_prob\n",
    "        self.crossover_num = args.crossover_num\n",
    "        self.mutation_num = args.mutation_num\n",
    "        self.parameters_limits = args.param_limits \n",
    "        self.min_parameters_limits = args.min_param_limits \n",
    "        # self.val_loader = val_loader\n",
    "        self.output_dir = output_dir\n",
    "        self.memory = []\n",
    "        self.vis_dict = dict()\n",
    "        self.keep_top_k = dict()\n",
    "        self.keep_top_k[self.select_num] = []\n",
    "        self.keep_top_k[50] = []\n",
    "        self.epoch = 0\n",
    "        self.candidates = []\n",
    "        self.top_accuracies = []\n",
    "        self.choices = choices\n",
    "        self.cand_params = []\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "\n",
    "        info = dict()\n",
    "        info['top_accuracies'] = self.top_accuracies\n",
    "        info['memory'] = self.memory\n",
    "        info['candidates'] = self.candidates\n",
    "        info['vis_dict'] = self.vis_dict\n",
    "        info['keep_top_k'] = self.keep_top_k\n",
    "        info['epoch'] = self.epoch\n",
    "        checkpoint_path = os.path.join(self.output_dir, \"checkpoint-{}.pth.tar\".format(self.epoch))\n",
    "        torch.save(info, checkpoint_path)\n",
    "        print('save checkpoint to', checkpoint_path)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        if not os.path.exists(self.checkpoint_path):\n",
    "            return False\n",
    "        info = torch.load(self.checkpoint_path)\n",
    "        self.memory = info['memory']\n",
    "        self.candidates = info['candidates']\n",
    "        self.vis_dict = info['vis_dict']\n",
    "        self.keep_top_k = info['keep_top_k']\n",
    "        self.epoch = info['epoch']\n",
    "\n",
    "        print('load checkpoint from', self.checkpoint_path)\n",
    "        return True\n",
    "\n",
    "    def set_config(self, config):\n",
    "        i = 0\n",
    "        for name, l in self.model.named_modules():\n",
    "            # if isinstance(l, torch.nn.Linear) and name!='head':\n",
    "            if isinstance(l, SuperScalableLinear) and name!='head':\n",
    "                l.eval_config = config[i]\n",
    "                i+=1\n",
    "\n",
    "    def get_param_tensor(self, config, in_feature, out_feature, name):\n",
    "        if 'A' in name or 'B' in name or 'C' in name:\n",
    "            if 'C' in name:\n",
    "                out_feature = in_feature\n",
    "                in_feature = 1\n",
    "            if 'LoRA' in config:\n",
    "                try:\n",
    "                    rank = int(config.split('_')[1])\n",
    "                except:\n",
    "                    rank = 16\n",
    "                param = out_feature*rank + in_feature*rank\n",
    "            elif 'vector' in config:\n",
    "                param = out_feature\n",
    "            elif 'constant' in config:\n",
    "                param = 1\n",
    "            elif 'none' in config:\n",
    "                param = 0\n",
    "            else:\n",
    "                raise ValueError\n",
    "        else:\n",
    "            if 'vector' in config:\n",
    "                param = out_feature\n",
    "            elif 'constant' in config:\n",
    "                param = 1\n",
    "            elif 'none' in config:\n",
    "                param = 0\n",
    "            else:\n",
    "                raise ValueError\n",
    "        return param\n",
    "    \n",
    "    def get_param(self, configs):\n",
    "        i = 0\n",
    "        params = 0\n",
    "\n",
    "        for n, l in self.model.named_modules():\n",
    "            if isinstance(l, SuperScalableLinear) and n != 'head':\n",
    "                out_channel = l.out_features\n",
    "                in_channel = l.in_features\n",
    "                for sup_tnsr in ['A', 'B', 'C', 'D', 'E']:\n",
    "                    # print(i, len(configs))\n",
    "                    params += self.get_param_tensor(configs[i][sup_tnsr], in_channel, out_channel, sup_tnsr)\n",
    "                i+=1\n",
    "        return params\n",
    "\n",
    "    def is_legal(self, cand):\n",
    "        assert isinstance(cand, tuple)\n",
    "        if str(cand) not in self.vis_dict:\n",
    "            self.vis_dict[str(cand)] = {}\n",
    "        info = self.vis_dict[str(cand)]\n",
    "        if 'visited' in info:\n",
    "            return False\n",
    "        n_parameters = self.get_param(configs=cand)\n",
    "        info['params'] =  n_parameters / 10.**6 \n",
    "        \n",
    "        if info['params'] > self.parameters_limits:\n",
    "            print('parameters limit exceed')\n",
    "            sys.stdout.flush()\n",
    "            return False\n",
    "\n",
    "        if info['params'] < self.min_parameters_limits:\n",
    "            print('under minimum parameters limit')\n",
    "            return False\n",
    "        \n",
    "        eval_acc = self.evaluate(config=cand)\n",
    "        info['acc'] = eval_acc\n",
    "        print(info['acc'])\n",
    "        info['visited'] = True\n",
    "\n",
    "        return True\n",
    "\n",
    "    def evaluate(self, config):\n",
    "        self.set_config(config)\n",
    "        self.model.eval()    \n",
    "        instructions = []\n",
    "        with open(data_path, 'r') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                data_point = {}\n",
    "                data_point[\"instruction\"] = 'What is the sentiment toward Bitcoin in the input sentence? [positive, negative, neutral]'\n",
    "                data_point[\"input\"] = data['text']\n",
    "                data_point[\"output\"] = data['label']\n",
    "                full_prompt = prompter.generate_prompt(\n",
    "                    data_point[\"instruction\"],\n",
    "                    data_point[\"input\"]\n",
    "                )\n",
    "                instructions.append({'context':full_prompt, 'target':data['label']})\n",
    "\n",
    "            # print(instructions[0])\n",
    "            start_time = time.time()\n",
    "            with torch.autocast(\"cuda\"):\n",
    "                with torch.no_grad():\n",
    "                    right = 0\n",
    "                    all = 0\n",
    "                    batch_size = 256\n",
    "                    input_texts = []\n",
    "                    targets = []\n",
    "                    for idx, item in enumerate(instructions[300:]):\n",
    "                        # feature = format_example(item)\n",
    "                        # input_text = feature[\"context\"]\n",
    "                        all = all + 1\n",
    "                        input_texts.append(item[\"context\"])\n",
    "                        targets.append(item[\"target\"])\n",
    "                    test_loader = DataLoader(input_texts, batch_size=batch_size)\n",
    "                    for batch_idx,batch in enumerate(test_loader):\n",
    "                        input_ids = tokenizer(batch, padding=True,return_tensors='pt').to('cuda')\n",
    "                        out = model.generate(\n",
    "                            **input_ids,\n",
    "                            temperature=0,\n",
    "                            return_dict_in_generate= True,\n",
    "                            output_scores=True,\n",
    "                            max_new_tokens = 1\n",
    "                        )\n",
    "                        seqs = out['sequences']\n",
    "                        scores = out['scores']\n",
    "                        # print(scores[0].shape)\n",
    "                        results = tokenizer.batch_decode(seqs)\n",
    "                        # print(results, '\\n\\n')\n",
    "                        # break\n",
    "                        for idx,res in enumerate(results):\n",
    "                            pred = res[res.find('Response') + 10:]\n",
    "                            target = targets[batch_idx*batch_size + idx]\n",
    "                            if target.find(pred) >= 0:\n",
    "                                right = right + 1\n",
    "                        # print(right,all,right/all)\n",
    "        return right/all\n",
    "        # self.model.eval()\n",
    "        # self.set_config(config)\n",
    "        # acc = Accuracy()\n",
    "        # for batch in self.val_loader:\n",
    "        #     x, y = batch[0].cuda(), batch[1].cuda()\n",
    "        #     out = self.model(x).data\n",
    "        #     acc.update(out.argmax(dim=1).view(-1), y, 1)\n",
    "\n",
    "        # return acc.result()[1]\n",
    "    \n",
    "    def update_top_k(self, candidates, *, k, key, reverse=True):\n",
    "        assert k in self.keep_top_k\n",
    "        print('select ......')\n",
    "        t = self.keep_top_k[k]\n",
    "        t += candidates\n",
    "        t.sort(key=key, reverse=reverse)\n",
    "        self.keep_top_k[k] = t[:k]\n",
    "\n",
    "    def stack_random_cand(self, random_func, *, batchsize=10):\n",
    "        while True:\n",
    "            cands = [random_func() for _ in range(batchsize)]\n",
    "            for cand in cands:\n",
    "                if str(cand) not in self.vis_dict:\n",
    "                    self.vis_dict[str(cand)] = {}\n",
    "                info = self.vis_dict[str(cand)]\n",
    "            for cand in cands:\n",
    "                yield cand\n",
    "\n",
    "    def get_random_cand(self):\n",
    "\n",
    "        cand_tuple = list()\n",
    "        depth = 64 ## 12 (depth) X 4 (layers)\n",
    "        for i in range(depth):\n",
    "            cand_tuple.append({'A':random.choice(self.choices['A']),\n",
    "                               'B':random.choice(self.choices['B']),\n",
    "                               'C':random.choice(self.choices['C']),\n",
    "                               'D':random.choice(self.choices['D']),\n",
    "                               'E':random.choice(self.choices['E'])})\n",
    "\n",
    "        return tuple(cand_tuple)\n",
    "\n",
    "    def get_random(self, num):\n",
    "        print('random select ........')\n",
    "        cand_iter = self.stack_random_cand(self.get_random_cand)\n",
    "        while len(self.candidates) < num:\n",
    "            cand = next(cand_iter)\n",
    "            if not self.is_legal(cand):\n",
    "                continue\n",
    "            self.candidates.append(cand)\n",
    "            print('random {}/{}'.format(len(self.candidates), num))\n",
    "        print('random_num = {}'.format(len(self.candidates)))\n",
    "\n",
    "    def get_mutation(self, k, mutation_num, m_prob):\n",
    "        assert k in self.keep_top_k\n",
    "        print('mutation ......')\n",
    "        res = []\n",
    "        iter = 0\n",
    "        max_iters = mutation_num * 10\n",
    "\n",
    "        def random_func():\n",
    "            cand = list(random.choice(self.keep_top_k[k]))\n",
    "            final = list()\n",
    "            for i in range(len(cand)):\n",
    "                final_layer = dict()\n",
    "                for key in ['A', 'B', 'C', 'D', 'E']:\n",
    "                    random_s = random.random()\n",
    "                    if random_s < m_prob:\n",
    "                        final_layer[key] = random.choice(self.choices[key])\n",
    "                    else:\n",
    "                        final_layer[key] = cand[i][key]\n",
    "                final.append(final_layer)\n",
    "            return tuple(final)\n",
    "\n",
    "        cand_iter = self.stack_random_cand(random_func)\n",
    "        while len(res) < mutation_num and max_iters > 0:\n",
    "            max_iters -= 1\n",
    "            cand = next(cand_iter)\n",
    "            if not self.is_legal(cand):\n",
    "                continue\n",
    "            res.append(cand)\n",
    "            print('mutation {}/{}'.format(len(res), mutation_num))\n",
    "\n",
    "        print('mutation_num = {}'.format(len(res)))\n",
    "        return res\n",
    "\n",
    "    def get_crossover(self, k, crossover_num):\n",
    "        assert k in self.keep_top_k\n",
    "        print('crossover ......')\n",
    "        res = []\n",
    "        iter = 0\n",
    "        max_iters = 10 * crossover_num\n",
    "\n",
    "        def random_func():\n",
    "            cand_1 = list(random.choice(self.keep_top_k[k]))\n",
    "            cand_2 = list(random.choice(self.keep_top_k[k]))\n",
    "            final = list()\n",
    "            for i in range(len(cand_1)):\n",
    "                final_layer = dict()\n",
    "                for key in ['A', 'B', 'C', 'D', 'E']:\n",
    "                    final_layer[key] = random.choice([cand_1[i][key], cand_2[i][key]])\n",
    "                final.append(final_layer)\n",
    "            return tuple(final)\n",
    "\n",
    "        cand_iter = self.stack_random_cand(random_func)\n",
    "        while len(res) < crossover_num and max_iters > 0:\n",
    "            max_iters -= 1\n",
    "            cand = next(cand_iter)\n",
    "            if not self.is_legal(cand):\n",
    "                continue\n",
    "            res.append(cand)\n",
    "            print('crossover {}/{}'.format(len(res), crossover_num))\n",
    "\n",
    "        print('crossover_num = {}'.format(len(res)))\n",
    "        return res\n",
    "\n",
    "    def search(self):\n",
    "        print(\n",
    "            'population_num = {} select_num = {} mutation_num = {} crossover_num = {} random_num = {} max_epochs = {}'.format(\n",
    "                self.population_num, self.select_num, self.mutation_num, self.crossover_num,\n",
    "                self.population_num - self.mutation_num - self.crossover_num, self.max_epochs))\n",
    "\n",
    "\n",
    "        self.get_random(self.population_num)\n",
    "\n",
    "        while self.epoch < self.max_epochs:\n",
    "            print('epoch = {}'.format(self.epoch))\n",
    "\n",
    "            self.memory.append([])\n",
    "            for cand in self.candidates:\n",
    "                self.memory[-1].append(cand)\n",
    "            \n",
    "            #updata top10\n",
    "            self.update_top_k(\n",
    "                self.candidates, k=self.select_num, key=lambda x: self.vis_dict[str(x)]['acc'])\n",
    "            #updata top50\n",
    "            self.update_top_k(\n",
    "                self.candidates, k=50, key=lambda x: self.vis_dict[str(x)]['acc'])\n",
    "\n",
    "            print('epoch = {} : top {} result'.format(\n",
    "                self.epoch, len(self.keep_top_k[50])))\n",
    "            tmp_accuracy = []\n",
    "            for i, cand in enumerate(self.keep_top_k[50]):\n",
    "                print('No.{} Top-1 val acc = {}, params = {}'.format(\n",
    "                    i + 1, self.vis_dict[str(cand)]['acc'], self.vis_dict[str(cand)]['params']))   \n",
    "                sys.stdout.flush()\n",
    "                tmp_accuracy.append(self.vis_dict[str(cand)]['acc'])\n",
    "            self.top_accuracies.append(tmp_accuracy)\n",
    "\n",
    "            mutation = self.get_mutation(\n",
    "                self.select_num, self.mutation_num, self.m_prob)\n",
    "            crossover = self.get_crossover(self.select_num, self.crossover_num)\n",
    "\n",
    "            self.candidates = mutation + crossover\n",
    "\n",
    "            self.get_random(self.population_num)\n",
    "\n",
    "            self.epoch += 1\n",
    "\n",
    "            self.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from argparse import ArgumentParser\n",
    "# parser = ArgumentParser()\n",
    "# parser.add_argument('--seed', type=int, default=1)\n",
    "# # parser.add_argument('--model', type=str, default='vit_base_patch16_224_in21k')\n",
    "# # parser.add_argument('--dataset', type=str, default='cifar')\n",
    "# parser.add_argument('--save_path', type=str, default='models/temp/')\n",
    "# parser.add_argument('--load_path', type=str, default='models/temp/')\n",
    "# parser.add_argument('--max-epochs', type=int, default=20)\n",
    "# parser.add_argument('--select-num', type=int, default=10)\n",
    "# parser.add_argument('--population-num', type=int, default=50)\n",
    "# parser.add_argument('--m_prob', type=float, default=0.2)\n",
    "# parser.add_argument('--crossover-num', type=int, default=25)\n",
    "# parser.add_argument('--epochs', type=int, default=30)\n",
    "# parser.add_argument('--mutation-num', type=int, default=25)\n",
    "# parser.add_argument('--param-limits', type=float, default=1.00)\n",
    "# parser.add_argument('--min-param-limits', type=float, default=0)\n",
    "# parser.add_argument('--rank', type=int, default=4)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# class Args:\n",
    "#     def __init__(self):\n",
    "#         self.seed = 1\n",
    "#         self.save_path = 'models/temp/'\n",
    "#         self.load_path = 'models/temp/'\n",
    "#         self.max_epochs = 20\n",
    "#         self.select_num = 10\n",
    "#         self.population_num = 50\n",
    "#         self.m_prob = 0.2\n",
    "#         self.crossover_num = 25\n",
    "#         self.epochs = 30\n",
    "#         self.mutation_num = 25\n",
    "#         self.param_limits = 1000.00\n",
    "#         self.min_param_limits = 0\n",
    "#         self.rank = 8\n",
    "\n",
    "# args = Args()\n",
    "# choices = dict()\n",
    "# choices['A'] = [f'LoRA_{lora_r}', 'vector', 'constant', 'none']\n",
    "# choices['B'] = choices['A']\n",
    "# choices['C'] = [f'LoRA_{lora_r}', 'vector', 'none']\n",
    "# choices['D'] = ['constant', 'none', 'vector']\n",
    "# choices['E'] = choices['D']\n",
    "\n",
    "# searcher = EvolutionSearcher(args, model, choices, args.save_path)\n",
    "# searcher.search()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
