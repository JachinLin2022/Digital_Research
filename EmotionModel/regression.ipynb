{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全量数据进行自回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error:0.0013970\n",
      "R2 Score:0.00300\n",
      "Corr:0.05481\n",
      "Mean Squared Error:0.0013964\n",
      "R2 Score:0.00346\n",
      "Corr:0.05886\n",
      "\n",
      "\n",
      "Mean Squared Error:0.0013849\n",
      "R2 Score:0.01348\n",
      "Corr:0.11610\n",
      "Mean Squared Error:0.0013718\n",
      "R2 Score:0.02283\n",
      "Corr:0.15111\n",
      "\n",
      "\n",
      "Mean Squared Error:0.0013823\n",
      "R2 Score:0.02256\n",
      "Corr:0.15021\n",
      "Mean Squared Error:0.0013605\n",
      "R2 Score:0.03799\n",
      "Corr:0.19491\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "scaler = MinMaxScaler()\n",
    "def test(lags,data_path, add_sentiment):\n",
    "    # print(data_path)\n",
    "    # 读取CSV文件\n",
    "    data = pd.read_csv(data_path)\n",
    "\n",
    "    # 加入发推量\n",
    "    data[['count']]= scaler.fit_transform(data[['count']])\n",
    "    data['metrics'] = data['metrics'] * data['count']\n",
    "\n",
    "    # data['Close'] = data['Close'].pct_change()\n",
    "    data['Close'] = np.log(data['Close']) - np.log(data['Close'].shift(1))\n",
    "    data['metrics'] = data['metrics'].diff()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    data = data[data['Close'].notna()]\n",
    "    # 提取\"Close\"和\"metrics\"列数据\n",
    "\n",
    "    price = data[\"Close\"]\n",
    "    metrics = data[\"metrics\"]\n",
    "    # lags = 7\n",
    "    price = price[lags:]\n",
    "\n",
    "    # 创建滞后外生变量的数据集\n",
    "    lagged_metrics = pd.DataFrame()\n",
    "    for lag in range(1, lags+1):\n",
    "        lagged_metrics[f\"metrics_lag{lag}\"] = metrics.shift(lag)\n",
    "\n",
    "    # 将滞后外生变量和原始数据合并\n",
    "    exog_data = pd.concat([lagged_metrics], axis=1)\n",
    "    exog_data = exog_data[exog_data[f'metrics_lag{lags}'].notna()]\n",
    "\n",
    "    # print(exog_data)\n",
    "    # print(price)\n",
    "\n",
    "\n",
    "    # time = 500\n",
    "    # train_data = price[:-time]\n",
    "    # test_data = price[-time:]\n",
    "    # train_sentiment = exog_data[:-time]\n",
    "    # test_sentiment = exog_data[-time:]\n",
    "    train_data = price\n",
    "    train_sentiment = exog_data\n",
    "    test_data = price\n",
    "    test_sentiment = exog_data\n",
    "    # print(len(train_data),len(train_sentiment))\n",
    "    # print(len(test_data),len(test_sentiment))\n",
    "\n",
    "    # print(test_data.values)\n",
    "    # 创建并拟合AutoReg模型\n",
    "    # add_sentiment = 1\n",
    "    if not add_sentiment:\n",
    "        model = AutoReg(train_data.values, lags=lags)\n",
    "    else:\n",
    "        model = AutoReg(train_data.values, lags=lags,exog=train_sentiment.values)\n",
    "    model_fit = model.fit()\n",
    "\n",
    "    # 预测未来100个时间点的股价\n",
    "    if add_sentiment:\n",
    "        # predictions = model_fit.predict(start=len(train_data), end=len(train_data)+len(test_data)-1,exog_oos=test_sentiment.values)\n",
    "        predictions = model_fit.predict(start=lags, end=len(train_data)-1,exog_oos=test_sentiment[lags:].values)\n",
    "    else:\n",
    "        # predictions = model_fit.predict(start=len(train_data), end=len(train_data)+len(test_data)-1)\n",
    "        predictions = model_fit.predict(start=lags, end=len(train_data)-1)\n",
    "    # 计算预测结果的均方误差\n",
    "    # print(predictions.values)\n",
    "    mse = mean_squared_error(test_data[lags:], predictions)\n",
    "    print('Mean Squared Error:{:.7f}'.format(mse))\n",
    "    # 计算R2\n",
    "    r2 = r2_score(test_data[lags:], predictions)\n",
    "    print('R2 Score:{:.5f}'.format(r2))\n",
    "    # 计算Corr\n",
    "    correlation = np.corrcoef(test_data[lags:], predictions)[0, 1]\n",
    "    print('Corr:{:.5f}'.format(correlation))\n",
    "    # 绘制预测结果\n",
    "\n",
    "    # plt.plot(train_data.index, train_data.values, label='Train Data')\n",
    "    # plt.plot(test_data[lags:].index, test_data[lags:].values, label='Test Data')\n",
    "    # plt.plot(test_data[lags:].index, predictions, label='Predictions')\n",
    "    # plt.legend()\n",
    "    # plt.xlabel('Date')\n",
    "    # plt.ylabel('Close Price')\n",
    "    # plt.title('Stock Price Prediction')\n",
    "    # plt.show()\n",
    "    \n",
    "    # granger因果检验\n",
    "    # plt.figure(figsize=(10, 5))\n",
    "    # df = pd.DataFrame({\"source\": predictions, \"target\": test_data[lags:]})\n",
    "    # results = []\n",
    "    # for maxlag in range(1, 30):\n",
    "    #     result = grangercausalitytests(df[['target','source']], maxlag=[maxlag])\n",
    "    #     results.append(result[maxlag][0]['ssr_ftest'][1])\n",
    "    # plt.plot(np.arange(1, 30), results, marker='o',label=f'lag-{lag}')\n",
    "    # plt.xlabel('Lag Order')\n",
    "    # plt.ylabel('Granger Causality Statistic')\n",
    "    # plt.title('Granger Causality Validation Curve')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "data_path = \"combined/sentiment_user_follower_count.csv\"\n",
    "\n",
    "for window in [1,7,14]:\n",
    "\n",
    "    test(window,data_path,0)\n",
    "    test(window,data_path,1)\n",
    "    print('\\n')\n",
    "# test(7,data_path,0)\n",
    "# test(7,data_path,1)\n",
    "# test(14,data_path,0)\n",
    "# test(14,data_path,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 1239 1239\n",
      "(1239, 7) (1239, 1)\n",
      "模型系数: [[-22080.21526114 -68730.38163782  13602.35825966 -21466.09587579\n",
      "  -29636.42457084 -53561.91845231   8009.02983583]]\n",
      "截距: [0.00098035]\n",
      "0.03402809672972292\n",
      "14 1232 1232\n",
      "(1232, 14) (1232, 1)\n",
      "模型系数: [[  30477.00654865   34431.26592122 -249206.48129477   83909.03748577\n",
      "   281800.66520192  303328.68538737  284191.76867069  235550.191522\n",
      "   156805.95418373  165809.83275846  127346.59879011   99490.91775708\n",
      "    34867.86831242   50845.72170783]]\n",
      "截距: [0.00091525]\n",
      "0.14355241617773587\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import torch.nn.init as init\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "def regression(data:pd.DataFrame,window,time):\n",
    "    data = data[data['Close'].notna()]\n",
    "    data = data[data['metrics'].notna()]\n",
    "\n",
    "\n",
    "    tensor_list = [data['metrics'].values[i:i+window] for i in range(len(data) - window-time)]\n",
    "    X = np.array(tensor_list)\n",
    "    # X = torch.from_numpy(X)\n",
    "\n",
    "    tensor_list = [data['Close'].values[i:i+1] for i in range(window,len(data)-time)]\n",
    "    Y = np.array(tensor_list)\n",
    "    # Y = torch.from_numpy(Y)\n",
    "\n",
    "\n",
    "    # tensor_list = [df['metrics'].values[i:i+window] for i in range(len(df) - window-time,len(df) - window)]\n",
    "    # X_test = np.array(tensor_list)\n",
    "    # # X_test = torch.from_numpy(X_test)\n",
    "\n",
    "    # tensor_list = [df['Close'].values[i:i+1] for i in range(len(df)-time,len(df))]\n",
    "    # Y_test = np.array(tensor_list)\n",
    "    # # Y_test = torch.from_numpy(Y_test)\n",
    "\n",
    "\n",
    "    print(window,len(X),len(Y))\n",
    "    # print(len(X_test),len(Y_test))\n",
    "\n",
    "    # 创建线性回归模型\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # 拟合模型\n",
    "    print(X.shape,Y.shape)\n",
    "    model.fit(X, Y)\n",
    "\n",
    "    # 打印模型系数\n",
    "    print('模型系数:', model.coef_)  # 斜率\n",
    "    print('截距:', model.intercept_)  # 截距\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    predicted_y = model.predict(X)\n",
    "    # print(predicted_y)\n",
    "\n",
    "\n",
    "    print(np.corrcoef(Y.ravel(), predicted_y.ravel())[0, 1])\n",
    "\n",
    "    # \n",
    "    # plt.figure(figsize=(10, 5))\n",
    "    # test_data = pd.DataFrame({\"source\": predicted_y.ravel(), \"target\": Y.ravel()})\n",
    "    # results = []\n",
    "    # for maxlag in range(1, 30):\n",
    "    #     result = grangercausalitytests(test_data[['target','source']], maxlag=[maxlag])\n",
    "    #     # print(result)\n",
    "    #     results.append(result[maxlag][0]['ssr_ftest'][1])\n",
    "    # plt.plot(np.arange(1, 30), results, marker='o',label=f'lag-{window}')\n",
    "    # plt.xlabel('Lag Order')\n",
    "    # plt.ylabel('Granger Causality Statistic')\n",
    "    # plt.title('Granger Causality Validation Curve')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df = pd.read_csv('combined/sentiment_all_feature.csv')\n",
    "# fear = pd.read_csv('Fear_and_Greed_Index.csv')\n",
    "# df = pd.merge(df,fear,left_on='date',right_on='timestamp',how='left')\n",
    "# df['metrics'] = df['metrics']\n",
    "\n",
    "# print(df)\n",
    "\n",
    "\n",
    "# df['Close'] = df['Close'].pct_change()\n",
    "\n",
    "# 加入发推量\n",
    "df[['count']]= scaler.fit_transform(df[['count']])\n",
    "df['metrics'] = df['metrics'] * df['count']\n",
    "\n",
    "df['Close'] = np.log(df['Close']) - np.log(df['Close'].shift(1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for diff_type in ['diff']:\n",
    "    if diff_type == 'pct_change':\n",
    "        df[['metrics']]= scaler.fit_transform(df[['metrics']])\n",
    "        df = df[df['metrics']!=0]\n",
    "        # print(df[df['metrics']==0])\n",
    "        df['metrics'] = df['metrics'].pct_change()\n",
    "    elif diff_type == 'diff':\n",
    "        df['metrics'] = df['metrics'].diff()\n",
    "    elif diff_type == 'log_return':\n",
    "        df[['metrics']]= scaler.fit_transform(df[['metrics']])\n",
    "        \n",
    "        df = df[df['metrics']!=0]\n",
    "        df['metrics'] = np.log(df['metrics']) - np.log(df['metrics'].shift(1))\n",
    "    else:\n",
    "        df['metrics'] = df['metrics']\n",
    "    \n",
    "    for window in [7, 14]:\n",
    "        regression(df.copy(), window,0)\n",
    "    # res,param,corrs = regression(df, window,0)\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # plt.plot(corrs[1], corrs[0],label='Corr')\n",
    "    # plt.xlabel('Epoth')\n",
    "    # plt.ylabel('Test Corr')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "    # print(\"{};{};{};{}\".format(window, diff_type, res, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 因果检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data = pd.read_csv('combined/sentiment_mean.csv')\n",
    "\n",
    "data['metrics'] =  data['metrics']\n",
    "data['value'] = data['Close']\n",
    "\n",
    "# window_size = 3 # 窗口大小\n",
    "# data['metrics'] = data['metrics'].rolling(window_size).mean()\n",
    "\n",
    "\n",
    "\n",
    "def show_test(data, lag,diff_type):\n",
    "\n",
    "\n",
    "    \n",
    "    data['metrics'] = data['metrics'].shift(lag)\n",
    "    \n",
    "    # window_size = 3 # 窗口大小\n",
    "    # data['metrics'] = data['metrics'].rolling(window_size).mean()\n",
    "    # data[['metrics']]= scaler.fit_transform(data[['metrics']])\n",
    "    # data['metrics_return'] = data['metrics'].pct_change()\n",
    "    # data = data[data['metrics_return'] < 9999]\n",
    "\n",
    "    # data['value_return'] = data['value'].pct_change()\n",
    "    # data['value_return'] = np.log(data['value']) - np.log(data['value'].shift(1))\n",
    "    # data = data[data['value_return'] < 9999]\n",
    "\n",
    "\n",
    "    if diff_type[0] == 'pct_change':\n",
    "        data[['metrics']]= scaler.fit_transform(data[['metrics']])\n",
    "        data = data[data['metrics']!=0]\n",
    "        data['metrics_return'] = data['metrics'].pct_change()\n",
    "    elif diff_type[0] == 'diff':\n",
    "        data['metrics_return'] = data['metrics'].diff()\n",
    "    elif diff_type[0] == 'log_return':\n",
    "        data[['metrics']]= scaler.fit_transform(data[['metrics']])\n",
    "        data = data[data['metrics']!=0]\n",
    "        data['metrics_return'] = np.log(data['metrics']) - np.log(data['metrics'].shift(1))\n",
    "\n",
    "    if diff_type[1] == 'pct_change':\n",
    "        data['value_return'] = data['value'].pct_change()\n",
    "    elif diff_type[1] == 'log_return':\n",
    "        data['value_return'] = np.log(data['value']) - np.log(data['value'].shift(1))\n",
    "    elif diff_type[1] == 'diff':\n",
    "        data['value_return'] = data['value'].diff()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    start = '2020-01-02'\n",
    "    end = '2023-05-30'\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data = data[(data['date']>= pd.Timestamp(start)) & (data['date'] <= pd.Timestamp(end))]\n",
    "    \n",
    "    data = data[data['metrics_return'].notna()]\n",
    "    data = data[data['value_return'].notna()]\n",
    "\n",
    "\n",
    "    source = 'metrics_return'\n",
    "    target = 'value_return'\n",
    "\n",
    "    print(data[source].corr(data[target]))\n",
    "\n",
    "    # 创建空的列表来存储结果\n",
    "    results = []\n",
    "\n",
    "    # print(len(data[source]))\n",
    "\n",
    "    # 尝试不同的滞后阶数并执行格兰杰因果性分析\n",
    "    for lag2 in range(1, 14):\n",
    "        result = grangercausalitytests(data[[target, source]], maxlag=[lag2])\n",
    "        # result = grangercausalitytests(data[[source, target]], maxlag=[lag])\n",
    "        # print(result)\n",
    "        results.append(result[lag2][0]['ssr_ftest'][1])\n",
    "\n",
    "    # 绘制验证曲线\n",
    "\n",
    "\n",
    "    # print(min(results))\n",
    "    lags = np.arange(1, 14)\n",
    "    \n",
    "\n",
    "    # print(results)\n",
    "    return lags, results\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# result_map = {}\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# for lag in range(10):\n",
    "#     lags, results = show_test(data.copy(), lag, ['pct_change','log_return'])\n",
    "#     plt.plot(lags, results, marker='o',label=f'lag-{lag}')\n",
    "#     result_map[lag] = min(results)\n",
    "# plt.xlabel('Lag Order')\n",
    "# plt.ylabel('Granger Causality Statistic')\n",
    "# plt.title('Granger Causality Validation Curve')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# print(result_map)\n",
    "\n",
    "for diff_type1 in ['pct_change','diff','log_return']:\n",
    "    for diff_type2 in ['pct_change','diff','log_return']:\n",
    "# for diff_type1 in ['pct_change']:\n",
    "#     for diff_type2 in ['pct_change']:\n",
    "        diff_type = []\n",
    "        diff_type.append(diff_type1)\n",
    "        diff_type.append(diff_type2)\n",
    "        result_map = {}\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for lag in range(5):\n",
    "            lags, results = show_test(data.copy(), lag, diff_type)\n",
    "            plt.plot(lags, results, marker='o',label=f'lag-{lag}')\n",
    "            result_map[lag] = min(results)\n",
    "\n",
    "\n",
    "        plt.xlabel('Lag Order')\n",
    "        plt.ylabel('Granger Causality Statistic')\n",
    "        plt.title('Granger Causality Validation Curve')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        print(diff_type, result_map)\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# 读取CSV文件\n",
    "data = pd.read_csv(\"sentiment.csv\")\n",
    "\n",
    "# 提取\"Close\"和\"metrics\"列数据\n",
    "\n",
    "\n",
    "def test(lags,data):\n",
    "    price = data[\"Close\"]\n",
    "    metrics = data[\"metrics\"]\n",
    "    # lags = 7\n",
    "    price = price[lags:]\n",
    "\n",
    "    # 创建滞后外生变量的数据集\n",
    "    lagged_metrics = pd.DataFrame()\n",
    "    for lag in range(1, lags+1):\n",
    "        lagged_metrics[f\"metrics_lag{lag}\"] = metrics.shift(lag)\n",
    "\n",
    "    # 将滞后外生变量和原始数据合并\n",
    "    exog_data = pd.concat([lagged_metrics], axis=1)\n",
    "    exog_data = exog_data[exog_data[f'metrics_lag{lags}'].notna()]\n",
    "\n",
    "    # print(exog_data)\n",
    "    # print(price)\n",
    "\n",
    "\n",
    "    time = 500\n",
    "    train_data = price[:-time]\n",
    "    test_data = price[-time:]\n",
    "    train_sentiment = exog_data[:-time]\n",
    "    test_sentiment = exog_data[-time:]\n",
    "\n",
    "\n",
    "    # print(len(train_data),len(train_sentiment))\n",
    "    # print(len(test_data),len(test_sentiment))\n",
    "\n",
    "    # print(test_data.values)\n",
    "    # 创建并拟合AutoReg模型\n",
    "    add_sentiment = 0\n",
    "    if not add_sentiment:\n",
    "        model = AutoReg(train_data.values, lags=lags)\n",
    "    else:\n",
    "        model = AutoReg(train_data.values, lags=lags,exog=train_sentiment.values)\n",
    "    model_fit = model.fit()\n",
    "\n",
    "\n",
    "    # 预测未来100个时间点的股价\n",
    "    if add_sentiment:\n",
    "        predictions = model_fit.predict(start=len(train_data), end=len(train_data)+len(test_data)-1,exog_oos=test_sentiment.values)\n",
    "    else:\n",
    "        predictions = model_fit.predict(start=len(train_data), end=len(train_data)+len(test_data)-1)\n",
    "    # 计算预测结果的均方误差\n",
    "    # print(predictions.values)\n",
    "    mse = mean_squared_error(test_data, predictions)\n",
    "    print('Mean Squared Error:', mse)\n",
    "    # 计算R2\n",
    "    r2 = r2_score(test_data, predictions)\n",
    "    print('R2 Score:', r2)\n",
    "    # 计算Corr\n",
    "    correlation = np.corrcoef(test_data, predictions)[0, 1]\n",
    "    print(correlation)\n",
    "    # 绘制预测结果\n",
    "\n",
    "    # plt.plot(train_data.index, train_data.values, label='Train Data')\n",
    "    plt.plot(test_data.index, test_data.values, label='Test Data')\n",
    "    plt.plot(test_data.index, predictions, label='Predictions')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.title('Stock Price Prediction')\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    add_sentiment = 1\n",
    "    if not add_sentiment:\n",
    "        model = AutoReg(train_data.values, lags=lags)\n",
    "    else:\n",
    "        model = AutoReg(train_data.values, lags=lags,exog=train_sentiment.values)\n",
    "    model_fit = model.fit()\n",
    "\n",
    "\n",
    "    # 预测未来100个时间点的股价\n",
    "    if add_sentiment:\n",
    "        predictions = model_fit.predict(start=len(train_data), end=len(train_data)+len(test_data)-1,exog_oos=test_sentiment.values)\n",
    "    else:\n",
    "        predictions = model_fit.predict(start=len(train_data), end=len(train_data)+len(test_data)-1)\n",
    "    # 计算预测结果的均方误差\n",
    "    # print(predictions.values)\n",
    "    mse = mean_squared_error(test_data, predictions)\n",
    "    print('Mean Squared Error:', mse)\n",
    "    # 计算R2\n",
    "    r2 = r2_score(test_data, predictions)\n",
    "    print('R2 Score:', r2)\n",
    "    # 计算Corr\n",
    "    correlation = np.corrcoef(test_data, predictions)[0, 1]\n",
    "    print(correlation)\n",
    "    # 绘制预测结果\n",
    "    # plt.plot(train_data.index, train_data.values, label='Train Data')\n",
    "    # plt.plot(test_data.index, test_data.values, label='Test Data')\n",
    "    plt.plot(test_data.index, predictions, label='Predictions')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.title('BTC Price Prediction')\n",
    "    # plt.show()\n",
    "test(7,data.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# 读取CSV文件\n",
    "data = pd.read_csv(\"sentiment_.csv\")\n",
    "data['Close'] = data['Close'].pct_change()\n",
    "\n",
    "\n",
    "\n",
    "# 加入发推量\n",
    "# data[['count']]= scaler.fit_transform(data[['count']])\n",
    "# data['metrics'] = data['metrics'] * df['count']\n",
    "\n",
    "data[['metrics']]= scaler.fit_transform(data[['metrics']])\n",
    "data = data[data['metrics']!=0]\n",
    "data['metrics'] = data['metrics'].pct_change()\n",
    "\n",
    "data = data[data['Close'].notna()]\n",
    "data = data[data['metrics'].notna()]\n",
    "# 提取\"Close\"和\"metrics\"列数据\n",
    "data = data.reset_index(drop=True)\n",
    "# print(data)\n",
    "\n",
    "\n",
    "\n",
    "def test(data,lags,time):\n",
    "    price = data[\"Close\"]\n",
    "    metrics = data['metrics']\n",
    "\n",
    "\n",
    "\n",
    "    # lags = 7\n",
    "    price = price[lags:]\n",
    "\n",
    "    # 创建滞后外生变量的数据集\n",
    "    lagged_metrics = pd.DataFrame()\n",
    "    for lag in range(1, lags+1):\n",
    "        lagged_metrics[f\"metrics_lag{lag}\"] = metrics.shift(lag)\n",
    "\n",
    "    # 将滞后外生变量和原始数据合并\n",
    "    exog_data = pd.concat([lagged_metrics], axis=1)\n",
    "    exog_data = exog_data[exog_data[f'metrics_lag{lags}'].notna()]\n",
    "\n",
    "\n",
    "    # exog_data = exog_data[lags:]\n",
    "\n",
    "    # print(exog_data)\n",
    "    # print(price)\n",
    "\n",
    "\n",
    "    # time = 800\n",
    "    train_data = price[:-time]\n",
    "    test_data = price[-time:]\n",
    "    train_sentiment = exog_data[:-time]\n",
    "    test_sentiment = exog_data[-time:]\n",
    "\n",
    "\n",
    "    add_sentiment = 1\n",
    "    # print(len(train_data),len(train_sentiment))\n",
    "    # 创建并拟合AutoReg模型\n",
    "    if add_sentiment:\n",
    "        model = AutoReg(train_data.values, lags=lags, exog=train_sentiment.values)\n",
    "    else:\n",
    "        model = AutoReg(train_data.values, lags=lags)\n",
    "    model_fit = model.fit()\n",
    "    # print(model_fit.params)\n",
    "\n",
    "    # 预测未来100个时间点的股价\n",
    "    if add_sentiment:\n",
    "        predictions = model_fit.predict(start=len(train_data), end=len(train_data)+len(test_data)-1,exog_oos=test_sentiment.values)\n",
    "    else:\n",
    "        predictions = model_fit.predict(start=len(train_data), end=len(train_data)+len(test_data)-1)\n",
    "    # 计算预测结果的均方误差\n",
    "    # print(predictions.values)\n",
    "    mse = mean_squared_error(test_data, predictions)\n",
    "    # print('Mean Squared Error:', mse)\n",
    "    # 计算R2\n",
    "    r2 = r2_score(test_data, predictions)\n",
    "    # print('R2 Score:', r2)\n",
    "    # 计算Corr\n",
    "    correlation1 = np.corrcoef(test_data, predictions)[0, 1]\n",
    "    # print('Corr:',correlation1)\n",
    "    # 绘制预测结果\n",
    "    # plt.plot(train_data.index, train_data.values, label='Train Data')\n",
    "    # plt.plot(test_data.index, test_data.values, label='Test Data')\n",
    "    # plt.plot(test_data.index, predictions, label='Predictions')\n",
    "    # plt.legend()\n",
    "    # plt.xlabel('Date')\n",
    "    # plt.ylabel('Close Price')\n",
    "    # plt.title('Stock Price Prediction')\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    add_sentiment = 0\n",
    "    # print(len(train_data),len(train_sentiment))\n",
    "    # 创建并拟合AutoReg模型\n",
    "    if add_sentiment:\n",
    "        model = AutoReg(train_data.values, lags=lags, exog=train_sentiment.values)\n",
    "    else:\n",
    "        model = AutoReg(train_data.values, lags=lags)\n",
    "    model_fit = model.fit()\n",
    "\n",
    "\n",
    "    # 预测未来100个时间点的股价\n",
    "    if add_sentiment:\n",
    "        predictions = model_fit.predict(start=len(train_data), end=len(train_data)+len(test_data)-1,exog_oos=test_sentiment.values)\n",
    "    else:\n",
    "        predictions = model_fit.predict(start=len(train_data), end=len(train_data)+len(test_data)-1)\n",
    "    # 计算预测结果的均方误差\n",
    "    # print(predictions.values)\n",
    "    mse = mean_squared_error(test_data, predictions)\n",
    "    # print('Mean Squared Error:', mse)\n",
    "    # 计算R2\n",
    "    r2 = r2_score(test_data, predictions)\n",
    "    # print('R2 Score:', r2)\n",
    "    # 计算Corr\n",
    "    correlation2 = np.corrcoef(test_data, predictions)[0, 1]\n",
    "    # print('Corr:',correlation2)\n",
    "    # 绘制预测结果\n",
    "    # plt.plot(train_data.index, train_data.values, label='Train Data')\n",
    "    # plt.plot(test_data.index, test_data.values, label='Test Data')\n",
    "    # plt.plot(test_data.index, predictions, label='Predictions')\n",
    "    # plt.legend()\n",
    "    # plt.xlabel('Date')\n",
    "    # plt.ylabel('Close Price')\n",
    "    # plt.title('Stock Price Prediction')\n",
    "    # plt.show()\n",
    "    return correlation1,correlation2\n",
    "# corr1,corr2 = test(data.copy(),7,100)\n",
    "corr1_l = []\n",
    "corr2_l = []\n",
    "for i in range(100,1200,100):\n",
    "    corr1,corr2 = test(data.copy(),7,i)\n",
    "    corr1_l.append(corr1)\n",
    "    corr2_l.append(corr2)\n",
    "    print(corr1,corr2)\n",
    "plt.plot(range(100,1200,100), corr1_l, label='add sentiment')\n",
    "plt.plot(range(100,1200,100), corr2_l, label='origin')\n",
    "plt.legend()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.title('Stock Price Prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
